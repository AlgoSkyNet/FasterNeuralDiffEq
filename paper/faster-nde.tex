\documentclass{article}

\usepackage{iclr2020_conference, times}
\iclrfinalcopy  % TODO: comment out for submission

\usepackage{pbox}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{multirow}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{graphicx,wrapfig,lipsum}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{empheq}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mathrsfs}  
\usepackage{amssymb}
\usepackage{thmtools}  % restatable
\usepackage{tikz}
\usepackage{nicefrac}
\usepackage{arydshln}
\usetikzlibrary{calc,positioning,matrix, decorations.pathreplacing}

\usepackage{color} 
\usepackage{listings} 
 
\definecolor{Code}{rgb}{0,0,0} 
\definecolor{Decorators}{rgb}{0.5,0.5,0.5} 
\definecolor{Numbers}{rgb}{0.5,0,0} 
\definecolor{MatchingBrackets}{rgb}{0.25,0.5,0.5} 
\definecolor{Keywords}{rgb}{0,0,1} 
\definecolor{self}{rgb}{0,0,0} 
\definecolor{Strings}{rgb}{0,0.63,0} 
\definecolor{Comments}{rgb}{0,0.63,1} 
\definecolor{Backquotes}{rgb}{0,0,0} 
\definecolor{Classname}{rgb}{0,0,0} 
\definecolor{FunctionName}{rgb}{0,0,0} 
\definecolor{Operators}{rgb}{0,0,0} 
\definecolor{Background}{rgb}{0.98,0.98,0.98} 
 
\lstdefinelanguage{Python}{ 
numbers=left, 
numberstyle=\footnotesize, 
numbersep=1em, 
xleftmargin=1em, 
framextopmargin=2em, 
framexbottommargin=2em, 
showspaces=false, 
showtabs=false, 
showstringspaces=false, 
frame=l, 
tabsize=4, 
% Basic 
basicstyle=\small,
backgroundcolor=\color{Background}, 
% Comments 
commentstyle=\color{Comments}\slshape, 
% Strings 
stringstyle=\color{Strings}, 
morecomment=[s][\color{Strings}]{"""}{"""}, 
morecomment=[s][\color{Strings}]{'''}{'''}, 
% keywords 
morekeywords={import,from,class,def,for,while,if,is,in,elif,else,not,and,or,print,break,continue,return,True,False,None,access,as,,del,except,exec,finally,global,import,lambda,pass,print,raise,try,assert}, 
keywordstyle={\color{Keywords}\bfseries}, 
% additional keywords 
morekeywords={[2]@invariant,pylab,numpy,np,scipy}, 
keywordstyle={[2]\color{Decorators}\slshape}, 
emph={self}, 
emphstyle={\color{self}\slshape}, 
% 
}  
\lstnewenvironment{python}[1][]{%
  \lstset{language=Python,#1}%
}{}

\hyphenpenalty=10000

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem*{thm-informal}{Theorem (Informal)}
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{corollary}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[thm]{Definition}
\newtheorem{remark}[thm]{Remark}
\newtheorem{example}[thm]{Example}

\newcommand{\expect}{\mathbb{E}}
\newcommand{\sig}{\mathrm{Sig}^D}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\littleo}{o}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\normal}[2]{\mathcal{N}\left(#1, #2\right)}
\newcommand{\set}[2]{\left\{#1\,\middle\vert\,#2\right\}}

\title{Train Neural Differential Equations 50\% Faster with 12 Lines of Code}
\author{Patrick Kidger\footnotemark[1]\qquad Ricky T. Q. Chen\footnotemark[2]\qquad Terry Lyons\footnotemark[1]\\
\footnotemark[1]\hspace{0.5em}Mathematical Institute, University of Oxford\\
\footnotemark[1]\hspace{0.5em}The Alan Turing Institute, The British Library\\
\footnotemark[2]\hspace{0.5em}Neverland\\
\texttt{\{kidger, tlyons\}@\hspace{0.8pt}maths.ox.ac.uk}\\
\texttt{rtqichen@\hspace{0.8pt}cs.toronoto.edu}}
\date{\today}
\begin{document}
\maketitle

\begin{abstract}
Neural differential equations may be trained by backpropagating gradients via the adjoint method, which is another differential equation typically solved using an adaptive-step-size numerical differential equation solver. A proposed step is accepted if its error, \emph{relative to some norm}, is sufficiently small; else it is rejected, the step is shrunk, and the process is repeated. Here, we demonstrate that the particular structure of the adjoint equations makes the usual choice of RMS norm unnecessarily stringent. By replacing it with a more appropriate (semi)norm, fewer steps are unnecessarily rejected and the backpropagation is made faster. This requires only minor code modifications. Experiments on Neural CDE (time series) models, Continuous Normalising Flow (generative) models and Hamiltonian NN (physical) models demonstrate performance improvements of 5\%--50\% depending on the ratio of parameters to hidden state.
\end{abstract}

\section{Introduction}
We begin by recalling the usual set-up for neural differential equations.

\subsection{Neural ordinary differential equations}
The most popular neural differential equation is arguably the Neural Ordinary Differential Equation (Neural ODE) of \citet{E2017, neural-odes}, which seeks to approximate a map $x \mapsto y$ by learning a function $f(\,\cdot\,,\,\cdot\,, \theta)$ and linear maps $\ell_1(\,\cdot\,,\phi)$, $\ell_2(\,\cdot\,,\psi)$ such that over some interval $[\tau, T]$,
\begin{equation}\label{eq:node}
z(\tau) = \ell_1(x, \phi),\qquad z(t) = z(\tau) + \int_\tau^t f(s, z(s), \theta) \mathrm{d} s\quad\text{and}\quad y \approx \ell_2(z(T), \psi),
\end{equation}
with $\theta$, $\phi$, $\psi$ learnt parameters.

\subsection{Applications}

Neural differential equations have to the best our knowledge three main applications:
\begin{enumerate}
\item Continuous Normalising Flows \cite{ffjord}, in which the overall model acts as map between probability distributions,
\item Irregular time series, either by interleaving with RNNs as in \citet{latent-odes} or by taking $f(t, z, \theta) = g(z, \theta) \frac{\dd X}{\dd t}(t)$ to be dependent on some time-varying input $X$ as in \citet{kidger2020neuralcde}.
\item Modelling physics, for which a differential equation based model is explicitly desired. \cite{TODO}
\end{enumerate}

\subsection{Adjoint equations}
The integral in equation \eqref{eq:node} may be backpropagated through either by backpropagating through the internal operations of a numerical solver, or by solving the backwards-in-time \emph{adjoint equations} with respect to some (scalar) loss $L$.
\begin{align}
a_z(T) &= \frac{\dd L}{\dd z(T)},\qquad &a_z(t) = a_z(T) - \int_T^t a_z(s) \cdot \frac{\partial f}{\partial z}(s, z(s), \theta) \,\dd s\qquad&\text{and}\hspace{0.6em} &\frac{\dd L}{\dd z(\tau)} = a_z(\tau),\nonumber\\
%
a_\theta(T) &= 0,\qquad &a_\theta(t) = a_\theta(T) - \int_T^t a_z(s) \cdot \frac{\partial f}{\partial \theta}(s, z(s), \theta) \,\dd s\qquad&\text{and}\hspace{0.6em} &\frac{\dd L}{\dd \theta} = a_\theta(\tau),\nonumber\\
%
a_t(T) &= \frac{\dd L}{\dd T},\qquad &a_t(t) = a_t(T) - \int_T^t a_z(s) \cdot \frac{\partial f}{\partial s}(s, z(s), \theta) \,\dd s\qquad&\text{and}\hspace{0.6em} &\frac{\dd L}{\dd \tau} = a_t(\tau),\label{eq:adjoint}
\end{align}

These equations are typically solved together as a joint system $a(t) = [a_z(t), a_\theta(t), a_t(t)]$. (They are already coupled; the latter two equations depend on $a_z$.) As additionally their integrands require $z(s)$, and as the results of the forward computation of equation \eqref{eq:node} are usually not stored, then the adjoint equations are typically additionally augmented by recovering $z$ according to the backwards-in-time equation
\begin{equation}\label{eq:backward}
z(t) = z(T) + \int_T^t f(s, z(s), \theta) \mathrm{d} s.
\end{equation}

\subsection{Contributions}
We demonstrate that the particular structure of the adjoint equations implies that numerical equation solvers will (with typical default arguments) take too many steps, that are too small, wasting time during backpropagation. Specifically, the accept/reject step of adaptive-step-size solvers is too stringent.

By applying a simple correction to account for this, we demonstrate that the number of steps needed to solve the adjoint equations can be reduced by as much as half. Factoring in the forward pass (which is unchanged), the overall training time may be improved by as much as 30\%, although naturally this varies by problem.

The correction itself involves very few lines of code, and at least with the \texttt{torchdiffeq} package (our chosen differential equation package), may be implemented with only 12 lines of code, making this an easy adjustment to make to any existing codebase.

\section{Method}
\subsection{Numerical solvers}
Both the forward pass given by equation \eqref{eq:node}, and the backward pass given by equations \eqref{eq:adjoint} and \eqref{eq:backward}, are solved by invoking a numerical differential equation solver. Our interest here is in adaptive-step-size solvers, and indeed a default choice for solving many equations is the adaptive-step-size Runge--Kutta 5(4) scheme of Dormand--Prince \citep{TODO}, for example as implemented by \texttt{dopri5} in the \texttt{torchdiffeq} package or \texttt{ode45} in MATLAB.

A full explanation of the internal operations of these solvers is beyond our scope here; the part of interest to us is the accept/reject scheme. Consider the case of solving the general ODE
\begin{equation*}
y(t) = y(\tau) + \int_\tau^t f(s, y(s)) \,\dd s,
\end{equation*}
with $y(t) \in \reals^d$.

Suppose for some fixed $t$ the solver has computed some estimate $\widehat{y}(t) \approx y(t)$, and it now seeks to take a step $\Delta > 0$ to compute $\widehat{y}(t + \Delta) \approx y(t + \Delta)$. A step is made, and some candidate $\widetilde{y}(t + \Delta)$ is generated. The solver additionally produces $y_\text{err} \in \reals^d$ representing an estimate of the numerical error made in each channel during that step.

Given some prespecified absolute tolerance $ATOL$ (for example $10^{-9}$), relative tolerance $RTOL$ (for example $10^{-6}$), and (semi)norm $\norm{\,\cdot\,} \colon \reals^d \to [0, \infty)$ (for example the RMS norm $\norm{y} = \sqrt{\frac{1}{d} \sum_{i = 1}^d y_i^2}$), then an estimate of the size of the equation is given by
\begin{equation}\label{eq:scale}
SCALE = ATOL + RTOL \cdot \max(\widehat{y}(t), \widetilde{y}(t + \Delta)) \in \reals^d,
\end{equation}
where the maximum is taken channel-wise, and the error ratio
\begin{equation}\label{eq:ratio}
r = \norm{\frac{y_\text{err}}{SCALE}}
\end{equation}
is then computed. If $r \leq 1$ then the error is deemed acceptable, the step is accepted and we take $\widehat{y}(t + \Delta) = \widetilde{y}(t + \Delta)$. If $r > 1$ then the error is deemed too large, the candidate $\widetilde{y}(t + \Delta)$ is rejected, and the procedure is repeated with a smaller $\Delta$.

Note the dependence on the choice of norm $\norm{\,\cdot\,}$: in particular this determines the relative importance of each channel towards the accept/reject criterion.

\subsection{Adjoint norms}
In equation \eqref{eq:adjoint}, we observe that $a_\theta(T) = 0$. When solving equation \eqref{eq:adjoint} numerically, this means for $t$ close to $T$ that the second term in equation \eqref{eq:scale} is small. As $ATOL$ is also typically very small, then $SCALE$ is small and the norm computed in equation \eqref{eq:ratio} becomes very large.

This implies that it becomes easy for the error ratio $r$ to violate $r \leq 1$, and it is easy for the step to be rejected.

The sign that something may be amiss with this strategy is that $a_\theta$ does not appear anywhere in the vector fields on equation \eqref{eq:adjoint}. Improving the accuracy of $a_\theta$ will not help improve the accuracy with which the ODE is solved (except incidentally, by increasing the resolution of the other channels, by having forced smaller step sizes).

It \emph{will} improve the accuracy with which we compute $\nicefrac{\dd L}{\dd \theta} = a_\theta(\tau)$, but the improvement is marginal: when $a_\theta$ grows larger for $t$ away from $T$, then $SCALE$ grows larger, steps are accepted more easily, the level of noise in the estimates grows, and the additional accuracy we gained at the start is lost. There was little reason to be so strict in the first place.

Thus, we come to our proposed strategy: when solving the adjoint equations equation \eqref{eq:adjoint}, choose a $\norm{\,\cdot\,}$ that scales down the effect in those channels corresponding to $a_\theta$.

Additionally, software may automatically compute $a_t$ even if the desired gradient is never used, for example because $\tau$ and $T$ are fixed: in such a scenario then much the same argument applies, and this channel may be scaled as well.

In practice, in our experiments, we scale $\norm{\,\cdot\,}$ all the way down by applying zero weight to the offending channels, so that $\norm{\,\cdot\,}$ is in fact a seminorm.

\subsection{Code}
Depending on the software package, the code for making this change can be trivial. For example, using PyTorch and \texttt{torchdiffeq} \citep{neural-odes}, the standard set-up
\begin{python}
import torchdiffeq

func = ...
y0 = ...
t = ...
torchdiffeq.odeint_adjoint(func=func, y0=y0, t=t)
\end{python}
is just changed to
\begin{python}
import torchdiffeq

def rms_norm(tensor):
    return tensor.pow(2).mean().sqrt()

def make_norm(state):
    state_size = state.numel()
    def norm(aug_state):
        y = aug_state[1:1 + state_size]
        adj_y = aug_state[1 + state_size:1 + 2 * state_size]
        return max(rms_norm(y), rms_norm(adj_y))
    return norm

func = ...    
y0 = ...
t = ...
torchdiffeq.odeint_adjoint(func=func, y0=y0, t=t, 
                           adjoint_options=dict(norm=make_norm(y0)))
\end{python}
and that's it.

To keep the remainder of this discussion software-agnostic, we defer further explanation of this specific code to Appendix \ref{appendix:torchdiffeq}.

\section{Experiments}

We demonstrate our proposed technique by comparing it against a conventionally-trained neural differential equation, across multiple regimes -- classification, time series, and generative.

TODO: discuss architectures, normalisation etc.

\subsection{Convolutional classification model}
TODO: experiments ongoing (effect does seem to be present)

\subsection{Neural Controlled Differential Equations}
This technique applies to more than just standard Neural ODE models. Consider the Neural Controlled Differential Equation (Neural CDE) model of \citet{kidger2020neuralcde}.

To recap, given some (potentially irregularly sampled) time series $\mathbf{x} = ((t_0, x_0), \ldots, (t_n, x_n))$, with each $t_i \in \reals$ the timestamp of the observation $x_i \in \reals^v$, let $X \colon [t_0, t_n] \to \reals^{1 + v}$ be an interpolation such that $X(t_i) = (t_i, x_i)$. For example $X$ could be a natural cubic spline.

Then take $f(t, z, \theta) = g(z, \theta) \frac{\dd X}{\dd t} (t)$ in a Neural ODE model, so that changes in $\mathbf{x}$ provoke changes in the vector field, and the model incorporates the incoming information $\mathbf{x}$. The output is $z(T)$ as in the Neural ODE, and the model may be thought of as a continuous-time RNN.

As this may be interpreted as an ODE (and indeed solved with the same software), then the same technique applies.

We apply a Neural CDE to the Speech Commands dataset \citep{TODO}, using the same hyperparameters as in \citet{kidger2020neuralcde} (see Appendix \ref{appendix:experiments} for details). This is a dataset of one-second audio recordings of spoken words such as `left', `right' and so on. We use 34975 time series corresponding to 10 spoken words so as to produce a balanced classification problem. We preprocess the dataset by computing mel-frequency cepstrum coefficients so that each time series is then regularly spaced with length 161 and 20 channels.

We demonstrate how the effect changes for varying tolerances by considering each pairs
\begin{align*}
ATOL = 10^{-6}, RTOL = 10^{-3},\\
ATOL = 10^{-7}, RTOL=10^{-4},\\
ATOL = 10^{-8}, RTOL=10^{-5},
\end{align*}
and for each such pair run five repeated experiments. 

See Table \ref{table:ncde}. We see that the accuracy of the model is unaffected by our proposed change. The backward pass requires roughly half the number of steps, and the overall training time (factoring in the forward pass that is left unchanged) is reduced by approximately 30\%.

\begin{table}[]
\centering
\caption{Across the test set: accuracy; number of function evaluations (NFE) to calculate an adjoint (backward) pass; total time to perform forward \emph{and} backward passes.\protect\footnotemark}\label{table:ncde}
\begin{tabular}{@{}lccc@{}}
\toprule
                                                                & \multicolumn{3}{c}{$ATOL= 10^{-6}, RTOL=10^{-3}$}                                                                      \\ \cmidrule(l){2-4} 
                                                                & Accuracy (\%)  & Backward NFE ($10^3$)       & \begin{tabular}[c]{@{}c@{}}Forward + Backward Time (seconds)\end{tabular} \\ \midrule
\begin{tabular}[c]{@{}l@{}}Neural CDE\\ (Default norm)\end{tabular} & \textbf{92.6 $\pm$ 0.4} & 21.5 $\pm$ 1.5767 & 275.4 $\pm$ 15.5                                                               \\
\begin{tabular}[c]{@{}l@{}}Neural CDE\\ (Our norm)\end{tabular} & \textbf{92.5 $\pm$ 0.5} & \textbf{12.5 $\pm$ 0.9625}    & \textbf{212.2 $\pm$ 11.4}                                                               \\ \bottomrule
\end{tabular}
\begin{tabular}{@{}lccc@{}}
\toprule
                                                                & \multicolumn{3}{c}{$ATOL= 10^{-7}, RTOL=10^{-4}$}                                                                      \\ \cmidrule(l){2-4} 
                                                                & Accuracy (\%)  & Backward NFE ($10^3$)       & \begin{tabular}[c]{@{}c@{}}Forward + Backward Time (seconds)\end{tabular} \\ \midrule
\begin{tabular}[c]{@{}l@{}}Neural CDE\\ (Default norm)\end{tabular} & TODO & TODO & TODO                                                               \\
\begin{tabular}[c]{@{}l@{}}Neural CDE\\ (Our norm)\end{tabular} & TODO & TODO    & TODO                                                               \\ \bottomrule
\end{tabular}
\begin{tabular}{@{}lccc@{}}
\toprule
                                                                & \multicolumn{3}{c}{$ATOL= 10^{-8}, RTOL=10^{-5}$}                                                                      \\ \cmidrule(l){2-4} 
                                                                & Accuracy (\%)  & Backward NFE ($10^3$)       & \begin{tabular}[c]{@{}c@{}}Forward + Backward Time (seconds)\end{tabular} \\ \midrule
\begin{tabular}[c]{@{}l@{}}Neural CDE\\ (Default norm)\end{tabular} & TODO & TODO & TODO                                                               \\
\begin{tabular}[c]{@{}l@{}}Neural CDE\\ (Our norm)\end{tabular} & TODO & TODO    & TODO                                                               \\ \bottomrule
\end{tabular}
\end{table}

\quad

% TODO: make sure this ends up on the right page
\footnotetext{For the avoidance of doubt: these backward passes were performed on the test set purely to evaluate their speed, and the calculated gradients were not used to train the model.}

\subsection{Continuous Normalising Flows}

\section{Related work}

\section{Conclusion}

\bibliography{faster-nde}
\bibliographystyle{iclr2020_conference}
\newpage
\appendix

\section{Code for \texttt{torchdiffeq}}\label{appendix:torchdiffeq}
TODO

\section{Experiments}\label{appendix:experiments}

\end{document}